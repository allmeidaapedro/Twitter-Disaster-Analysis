{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise de tweets: Desastre ou não?\n",
    "- Este é um projeto de ciência de dados de ponta a ponta (da coleta de dados ao deploy), que tem como objetivo <b>analisar</b> um conjunto de <b>tweets</b> e <b>prever a probabilidade de um tweet estar relacionado a um desastre real.</b> Portanto, trata-se de um problema de classificação binária (aprendizado supervisionado, conjunto de dados rotulado) no qual a variável dependente é 1 para o caso de tweets relacionados a desastres reais e 0 para tweets não relacionados.\n",
    "- O pipeline de solução, com base no framework <B>CRISP-DM</b>, consiste dos seguintes <b>passos:</b><br>\n",
    "\n",
    "    <b>0. Entendimento do problema de negócio.</b><br>\n",
    "    <b>1. Entendimento dos dados.</b><br>\n",
    "    <b>2. Preparação dos dados.</b><br>\n",
    "    <b>3. Modelagem.</b><br>\n",
    "    <b>4. Avaliação.</b><br>\n",
    "    <b>5. Deploy/Implantação.</b><br>\n",
    "\n",
    "- Neste notebook, será realizada a <b>modelagem para a predição da probabilidade de um tweet estar relacionado a um desastre</b>, cobrindo os passos 3 a 5 do framework supracitado. O objetivo da modelagem consiste em <b>construir um modelo</b> capaz de <b>prever essa chance acuradamente</b> e criar uma <b>Flask API</b>, de forma que seja possível prever a probabilidade de um tweet estar associado a um desastre fornecendo-o em texto.\n",
    "- Aqui, adotaremos a abordagem de <b>prever a probabilidade</b>, levando em consideração a <b>informação de confiança.</b> Isso nos permite avaliar quão provável é que um determinado tweet represente uma catástrofe real. Tal abordagem facilita a gestão de risco por parte das agências mencionadas no problema de negócio em 'eda.ipynb', possibilitando a <b>priorização da atenção</b> para tweets com maior probabilidade. Além disso, ><b>reduz a propagação de notícias falsas</b>, uma vez que rótulos binários poderiam classificar erroneamente vários tweets, especialmente ao utilizar pontos de corte mais baixos para a classificação (trade-off precision-recall).\n",
    "- Nesse sentido, <B>métricas</b> como ROC-AUC, PR-AUC, Brier Score e KS serão <b>priorizadas.</b> Entretanto, olharemos para diversas outras. Mesmo adotando o critério probabilístico, é interessante obter um bom recall. O recall pode ser interpretado como o alcance, e de fato, é melhor que \"alcancemos\" o maior número de desastres reais possível, isto é, que o modelo seja capaz de identificar uma grande parte dos tweets de fato associados a desastres.\n",
    "- O pipeline de modelagem consiste em:\n",
    "- 0. Limpeza e feature engineering.\n",
    "- 1. Split dos dados em treino e teste, estratificado.\n",
    "- 2. Pré-processamento de dados.\n",
    "- 3. Comparação de diversos modelos através de validação cruzada k-fold estratificada.\n",
    "- 4. Tunagem de hiperparâmetros do modelo com class_weight e otimização bayesiana.\n",
    "- 5. Avaliação final no conjunto de testes.\n",
    "- 6. Deploy.\n",
    "- Cada etapa e cada decisão tomada é abordada em detalhes abaixo. Os insights obtidos na eda guiarão as decisões daqui para frente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peedr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peedr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\peedr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\peedr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peedr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peedr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\peedr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\peedr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation and visualization.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Cleaning and preparation.\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Utils.\n",
    "from src.modelling_utils import *\n",
    "\n",
    "# Definições de cores -> todas estão numa escala de mais escura para mais clara.\n",
    "CINZA1, CINZA2, CINZA3 = '#231F20', '#414040', '#555655'\n",
    "CINZA4, CINZA5, CINZA6 = '#646369', '#76787B', '#828282'\n",
    "CINZA7, CINZA8, CINZA9 = '#929497', '#A6A6A5', '#BFBEBE'\n",
    "AZUL1, AZUL2, AZUL3, AZUL4 = '#174A7E', '#4A81BF', '#94B2D7', '#94AFC5'\n",
    "VERMELHO1, VERMELHO2, VERMELHO3, VERMELHO4, VERMELHO5 = '#DB0527', '#E23652', '#ED8293', '#F4B4BE', '#FBE6E9'\n",
    "VERDE1, VERDE2 = '#0C8040', '#9ABB59'\n",
    "LARANJA1 = '#F79747'\n",
    "AMARELO1, AMARELO2, AMARELO3, AMARELO4, AMARELO5 = '#FFC700', '#FFCC19', '#FFEB51', '#FFE37F', '#FFEEB2'\n",
    "BRANCO = '#FFFFFF'\n",
    "\n",
    "# Visualize all the columns.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "# Filter warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coletando novamente os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data.\n",
    "train_path = '../input/train.csv'\n",
    "test_path = '../input/test.csv'\n",
    "sample_path = '../input/sample_submission.csv'\n",
    "df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "sample_df = pd.read_csv(sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rápida visualização e informação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1118</td>\n",
       "      <td>blew%20up</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>@YahooSchwab easy way to look good after the R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1558</td>\n",
       "      <td>bomb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@dopeitsval ahh you're bomb baby ??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4830</td>\n",
       "      <td>evacuation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Run out evacuation hospital indexing remedial ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3674</td>\n",
       "      <td>destroy</td>\n",
       "      <td>Trackside California</td>\n",
       "      <td>Wow Crackdown 3 uses multiple servers in multi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5368</td>\n",
       "      <td>fire%20truck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wild night in the village of pugwash every fir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id       keyword              location   \n",
       "0  1118     blew%20up          Brooklyn, NY  \\\n",
       "1  1558          bomb                   NaN   \n",
       "2  4830    evacuation                   NaN   \n",
       "3  3674       destroy  Trackside California   \n",
       "4  5368  fire%20truck                   NaN   \n",
       "\n",
       "                                                text  target  \n",
       "0  @YahooSchwab easy way to look good after the R...       0  \n",
       "1                @dopeitsval ahh you're bomb baby ??       0  \n",
       "2  Run out evacuation hospital indexing remedial ...       1  \n",
       "3  Wow Crackdown 3 uses multiple servers in multi...       0  \n",
       "4  wild night in the village of pugwash every fir...       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataset possui 7613 linhas e 5 colunas.\n"
     ]
    }
   ],
   "source": [
    "print(f'O dataset possui {df.shape[0]} linhas e {df.shape[1]} colunas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valores nulos e duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>2533</td>\n",
       "      <td>33.272035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>61</td>\n",
       "      <td>0.801261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count        pct\n",
       "location   2533  33.272035\n",
       "keyword      61   0.801261\n",
       "id            0   0.000000\n",
       "text          0   0.000000\n",
       "target        0   0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isna_df = df.isna().sum().to_frame().rename(columns={0: 'count'})\n",
    "isna_df['pct'] = isna_df['count'] / len(df) * 100\n",
    "isna_df.sort_values(by=['pct'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observações duplicadas: 0.\n",
      "Ids duplicados: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Observações duplicadas: {df.duplicated().sum()}.')\n",
    "print(f'Ids duplicados: {df.id.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Limpeza\n",
    "- Antes dos split, é necessário limpar os dados, aplicando as mesmas transformações realizadas na eda.\n",
    "- 1. Remoção de links, tags html, @s de usuários, pontuações, caracteres especiais, números e stopwords (palavras sem natureza informativa) com o objetivo de não prejudicar a semântica, não introduzir ruído, e diminuir a dimensionalidade. Além disso, há uma questão ética em não usar o nome dos usuários. Tudo isso feito através de expressões regulares, que são padrões de texto que permitem realizar buscas complexas e operações de manipulação em strings. Um cheat sheet de referência será usado: https://www.datacamp.com/cheat-sheet/regular-expresso.\n",
    "- 2. Conversão das palavras a lowercase para padronizá-las e reduzir a dimensionalidade.\n",
    "- 3. Lematização, para manter a coerência linguística das palavras ao reduzi-las a suas raízes ou lemas. Utiliza-se part of speech tags aqui. Stemming não é aplicado pois, apesar de a lematização ser mais custosa, desejo demonstrar mais conhecimento e técnica. Além disso, no deploy, processaremos um único tweet por vez, não havendo gargalos significativos.\n",
    "- 4. Para a modelagem, estamos interessados apenas na coluna contendo os tweets como variável independente, portanto, essa será separada das outras.\n",
    "- Estes passos foram encapsulados em uma função 'clean_text' em 'modelling_utils.py', arquivo que contém funções úteis para a modelagem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@YahooSchwab easy way to look good after the R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@dopeitsval ahh you're bomb baby ??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run out evacuation hospital indexing remedial ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow Crackdown 3 uses multiple servers in multi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wild night in the village of pugwash every fir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  @YahooSchwab easy way to look good after the R...       0\n",
       "1                @dopeitsval ahh you're bomb baby ??       0\n",
       "2  Run out evacuation hospital indexing remedial ...       1\n",
       "3  Wow Crackdown 3 uses multiple servers in multi...       0\n",
       "4  wild night in the village of pugwash every fir...       1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separando apenas as colunas text e target para a modelagem.\n",
    "clean_df = df[['text', 'target']].copy()\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           easy way look good ray rice fiascothat blow\n",
       "1                                   ahh youre bomb baby\n",
       "2     run evacuation hospital index remedial angiopl...\n",
       "3     wow crackdown us multiple server multiplayer u...\n",
       "4     wild night village pugwash every fire truck to...\n",
       "5                                bed look like war zone\n",
       "6               fund need rescue abandon cocker spaniel\n",
       "7            burn building mean like burnt black church\n",
       "8                                               survive\n",
       "9     emergency drink water plan download guide engl...\n",
       "10    california fire rage forest service sound alar...\n",
       "11               something place prevent skynet perhaps\n",
       "12           new smp ignition knock detonation sensor k\n",
       "13    memorial unveil travis county deputy kill sept...\n",
       "14    live balance life balance fear # allah hope me...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acessando o resultado.\n",
    "clean_df = nlp_data_cleaning(clean_df)\n",
    "clean_df['clean_text'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
