{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keith's nlp tutorial:\n",
    "\n",
    "Bag of Words:\n",
    "- Easiest way to take text and conver it into numerical vectors.\n",
    "- Represents a text as a non-ordered word set.\n",
    "- Ignores gramatical structure and order of the text's words. Takes only frequency into account.\n",
    "- 1. Tokenization: Split the text into individual words/sentences (tokens), removes ponctuation, applies lowercase, removes stopwords (non-content words that primarily has only grammatical function).\n",
    "- 2. Vocabulary construction: Creates a unique vocabulary - all the unique words present in the corpus texts.\n",
    "- 3. Vectorization: Represents each text as a vector, where each position corresponds to a vocabulary word and a value representing the word's count at the respective text.\n",
    "- It can be binary.\n",
    "- \"A good classic classification model for text is often a linear svm\".\n",
    "- Do just transform on test, we already have the vectorizer.\n",
    "- As we add more training utterances we fit a more powerful bag of words model we will get oftentimes. We might build a such big dictionary that it will be hard to process the model, maybe we can take only the top N words that occur.\n",
    "- n-gramas:\n",
    "    - sequencias contiguas de n elementos consecutivos em uma amostra de texto\n",
    "    - unigrama = uma unica palavra = 'gato'\n",
    "    - bigrama = sequencia de duas palavras consecutivas = 'o gato'\n",
    "    - trigrama = sequencia de tres palavras consecutivas = 'o gato dorme'\n",
    "    - pode capturar relações semânticas mais complexas, por exemplo, \"great\" (unigrama) é diferente de \"not great\" (bigrama)\n",
    "    - pode adicionar viés também no caso de n-gramas com poucas observações, devemos ter cuidado e testar\n",
    "- \"Bag of Words is great on the stuff it is trained on, but if it hasn't seen a word then it just fails miserably\".\n",
    "- \"That's a disavantadge, in human language we can say things in so many different ways\".\n",
    "\n",
    "Word Vectors:\n",
    "- Another way to represent text as a numerical vector.\n",
    "- Captures the semantic meaning of a word in a vector.\n",
    "- \"Imagine we have words red, blue and yellow. In word of vectors we are trying to have in our vector space the similar words to be mapped in a similar spot of that vector space, so red, blue and yellow are all colors and should be mapped somewhere similar\".\n",
    "- Look at windows of words (context window) and selectively look at different tokens in that context window and based on that token we will utilize the surrounding tokens to kind of figure out the context of each token and kind of start developing a meaning of each token.\n",
    "- \"If we read enough text we can start to see relationships between words. For example, one relationship we might seem over the time is book often appears close to read so now we can associate book and read into a similar spot of the vector space\".\n",
    "- If we do nlp(\"phrase\") and then do .vector, it will take all the individual word embeddings and average them together.\n",
    "- These word vectors have several hundreds dimensions, but they can capture the information we need.\n",
    "- Each of the items in docs list is a word vector representation of the sentences passed.\n",
    "\n",
    "Regexes:\n",
    "- Pattern matching of strings in python\n",
    "- Password checkers, phone numbers, emails and more!\n",
    "\n",
    "Stemming/Lemmmatization:\n",
    "- Techniques to normalize text.\n",
    "- Stemming follows an algorithm and it is not guaranteed to give you a actual physical true english word. \n",
    "- Lemmatizing uses a dictionary and makes sure that everything it outputs is an actual english word.\n",
    "- reading -> read, books -> book, stories -> stori for stemming, story for lemmatizing.\n",
    "- NLTK library.\n",
    "- In our bag of words model, stemming would help improving accuracy.\n",
    "- The lemmatizer expects, for each of the words to be lemmatized, the part of speech. By default, each token is a Noun. Books become book, but reading does not become read.\n",
    "- Part of speech tagging is necessary.\n",
    "\n",
    "Stopwords Removal\n",
    "- The set of most common words in english.\n",
    "- We want to strip those out of our phrases because they don't add much meaning to our sentences.\n",
    "- This, that, he, it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
